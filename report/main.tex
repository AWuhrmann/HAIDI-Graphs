\documentclass[a4paper,12pt]{article}
\usepackage[dvipsnames]{xcolor}
\usepackage{emoji}
\usepackage{enumitem}
\usepackage{natbib}
\usepackage[backend=biber]{biblatex}
\addbibresource{references.bib}

\title{\textbf{Project Status Report} \emoji{rocket}}
\author{Arthur Wuhrmann}
\date{\today}
\usepackage{hyperref}

\begin{document}

\maketitle

\paragraph{Goal (to refine)} We discussed on 21/02, and it seems like many approaches could be taken and it is important to be specific about what I want to achieve. 
The goal is to perform exact or near-exact matching from the output of the LLM to its training data. We will first cover a word-specific topic to make the task simpler, like chemistry. Different prompt can be used to try to match some input, like using "According to academia, [...]" as a prefix. The matching can be done either on Google, or by indexing some parts of the training data (if known). 

This means we will not look for general model biases (like biased representations of jobs etc.), because it is less tractable. We might also limit ourselves to certain training data / models, for computational reasons.

\section*{Reading \emoji{books}}
\subsection*{Papers Read}
\begin{itemize}
    \item \textbf{Paper 1:} \citeauthor{bender_dangers_2021} (\citeyear{bender_dangers_2021}) - \textit{\citetitle{bender_dangers_2021}} \emoji{white-check-mark}
        \item \textbf{Paper 2:} \citeauthor{elazar_whats_2024} (\citeyear{elazar_whats_2024}) - \textit{\citetitle{elazar_whats_2024}} \emoji{white-check-mark}

    \item \textbf{Paper 3:} \citeauthor{marone_data_2023} (\citeyear{marone_data_2023}) - \textit{\citetitle{marone_data_2023}} \emoji{white-check-mark}

    \item \textbf{Paper 4:} \citeauthor{mirzadeh_gsm-symbolic_2024} (\citeyear{mirzadeh_gsm-symbolic_2024}) - \textit{\citetitle{mirzadeh_gsm-symbolic_2024}} \emoji{white-check-mark}
    \item \textbf{Paper 5:} \citeauthor{lee_deduplicating_2022} (\citeyear{lee_deduplicating_2022}) - \textit{\citetitle{lee_deduplicating_2022}}: Shows different approaches to removing duplicates \emoji{white-check-mark}

\end{itemize}


\subsection*{Main Takeaways}
\begin{itemize}
    \item Type of indexing to try contains Bloom filters\cite{marone_data_2023}, suffix arrays \cite{lee_deduplicating_2022}, grepping\cite{marone_data_2023}, Jaccard index for partial matching \cite{lee_deduplicating_2022}
    \item Unsafe models to do preliminary analysis : \href{https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard}{HuggingFace link}. However, harder to find a pair unsafe model - dataset, where the dataset is properly labelled and where some parts only can be downloaded (like the chemistry part)
    \item However, this approach would also mean that we probably won't have exact matching. Thus, we should be careful and 
\end{itemize}



\section*{Coding \emoji{computer}}
Available on \href{https://github.com/Reliable-Information-Lab-HEVS/HAIDI-Graphs}{GitHub}. 
\subsection*{To be done until Mar 01 / Mar 08}
\begin{itemize}
    \item [\color{ForestGreen}{DONE}] Got model inference on cluster (Put right parameters, as Andrei suggested)
    \item [\color{ForestGreen}{DONE}] Create diverse dangerous prompts. \textit{However, we encountered some problems because the model (Mistral 7B v.03) often rephrases mispelling the words, for example \texttt{Novichock} instead of \texttt{Novichoc}. We think this might be due to the tokenization, or to the context given to the model.}
    \item [\color{Goldenrod}WIP] Get the perplexity for each token of the answer and make nice visualization plots.
    \item [\color{BrickRed}SOON] Isolate potential matches from the outputs
    \item [\color{BrickRed}SOON] Implement (near-)grep and bloom filters on the chemistry part of The Pile
    \item [\color{BrickRed}SOON] Perform matching and evaluate the performance
\end{itemize}

\section*{Writing \emoji{pencil}}
\textit{Not yet}

\section*{Meetings}
\begin{itemize}
    \item \textbf{2025/02/10}: First meeting. Talked about the project in general, mentioned what techniques existed. We agreed that we should try a smaller size project, and might have to target smaller datasets, only on specific areas for example.  
    \item \textbf{2025/02/21}: We reviewed some of the literature, and try to specify clearly which goal we would achieve (see the beginning of the report). We stated clear next steps, i.e. try a first implementation. Infer some dangerous prompt, try to find it back. For now, "simple" technique like (near-)grepping or bloom filters. Also, look at the perplexity of the model when answering.  
    \item \textbf{2025/02/27}: Meeting with Andrei, Alexander, Anastasiia, Ines and Leo. We cleared differentiated the points of our projects, and got some feedback. I need to log more what I find. We mentioned some of the difficulties I would encounter (non-exact matching, size of data, ...)
    \item \textbf{2025/03/03}: Meeting with Anastasiia. Clarifying what I was working on, and making clearer objectives for the project. Mentioned some of the results obtained, with outputs that seem to lack technical chemical words. We will try to cover enough probability space to end up on low perplexity technical output that could be used to map to the original training data.
    \item \textbf{2025/03/10}: Hey so I talked with @Anastasiia , and we discussed a bit the next steps.
We have identified a few concerns about the current topic / prompts we are using : 

    I am not able to quickly identified if the answer of the model is dangerous or not, and though we have help from chemists this is not optimal for me...
    It seems like the book that @Andrei found on chemicals is not in The Pile, at least wimbd (What's In My Big Data) does not find some extracts @Anastasiia tried.
    In general, it is hard to know when choosing a prompt / topic whether this will be in the training data or not.


We identified different ideas that could be followed : 

    Training data -> Prompt (Basically the idea Andrei had) : We know something dangerous is in the dataset and we use this as a topic
    Prompt -> Training data : We get the model to say something dangerous (that we know from our knowledge), and deduce this has to be somewhere in the dataset


In the first case, we discussed that I could switch to a different chemical that has to be there, and we thought some drugs used in medicine could be good, like morphine / ketamine / fentanyl.
Otherwise, we could switch topic and go to for example computer science / cyber defense.

This has two/three advantages for me :

    I know more on that, so I can more easily spot if something has some potential harm or is just BS
    The data is quite easily available (this could be the case also for medicine...) and I am pretty sure the pile contains some harmful code at least somewhere.


I would appreciate some feedback on this ! We will have a quick talk again on wednesday to make a decision on this.
\end{itemize}

\printbibliography
\end{document}

